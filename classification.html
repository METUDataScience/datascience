<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Classification and Error Measurements</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Introduction to Data Science</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Tutorials
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="installation.html">R/RStudio Installation</a>
    </li>
    <li>
      <a href="azure_notebooks.html">Azure Notebooks</a>
    </li>
    <li>
      <a href="intro_to_r.html">Introduction to R</a>
    </li>
    <li>
      <a href="amelia.html">Missing Data Imputation</a>
    </li>
    <li>
      <a href="associationMining.html">Association Mining</a>
    </li>
    <li>
      <a href="classification.html">Classification</a>
    </li>
    <li>
      <a href="regression.html">Regression Analysis</a>
    </li>
    <li>
      <a href="timeSeries.html">Time Series Analysis</a>
    </li>
    <li>
      <a href="visualization.html">Visualization</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://mehmetaliakyol.com">
    <span class="fa fa-question fa-lg"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Classification and Error Measurements</h1>

</div>


<p><strong>Objectives</strong>:</p>
<p>The objective of this document is to give a brief introduction to classification methods and model evaluation. After completing this tutorial you will be able to:</p>
<ul>
<li>Generate classification models</li>
<li>Calculate accuracy, sensitivity and specificity values</li>
<li>Evaluate model performance</li>
<li>Calculate AUC</li>
</ul>
<p>Let’s load the data:</p>
<pre class="r"><code>data(iris) 
head(iris)</code></pre>
<pre><code>##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa</code></pre>
<pre class="r"><code>shuffleIris &lt;- iris[sample(nrow(iris)),] #Shuffle the dataset 
trainIris &lt;- shuffleIris[1:100,] #Subset the training set 
testIris &lt;- shuffleIris[101:150,-5] #Subset the test set without the class column 
testClass &lt;- shuffleIris[101:150,5] #Get test classes into a separate vector</code></pre>
<div id="k-nearest-neighbors-classification" class="section level2">
<h2>k-Nearest Neighbors Classification</h2>
<pre class="r"><code>require(class)
predClass &lt;- knn(trainIris[,-5],testIris, trainIris[,5], k = 5) #knn(trainvariables, testvariables, trainclasses, k)
require(caret)
confusionMatrix(testClass, predClass)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##             Reference
## Prediction   setosa versicolor virginica
##   setosa         17          0         0
##   versicolor      0         17         2
##   virginica       0          1        13
## 
## Overall Statistics
##                                           
##                Accuracy : 0.94            
##                  95% CI : (0.8345, 0.9875)
##     No Information Rate : 0.36            
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.9096          
##                                           
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: setosa Class: versicolor Class: virginica
## Sensitivity                   1.00            0.9444           0.8667
## Specificity                   1.00            0.9375           0.9714
## Pos Pred Value                1.00            0.8947           0.9286
## Neg Pred Value                1.00            0.9677           0.9444
## Prevalence                    0.34            0.3600           0.3000
## Detection Rate                0.34            0.3400           0.2600
## Detection Prevalence          0.34            0.3800           0.2800
## Balanced Accuracy             1.00            0.9410           0.9190</code></pre>
<p>This has a pretty high accuracy. This is partly due to how clean our data is.</p>
<p>The confusion matrix gives us a table that tells the overlap between true class and the predicted class. The columns give us the true class while the rows give us the predicted ones. Take a look at the <code>virginica</code> column in the confusion matrix. One instance of data that is actually <code>virginica</code> is classified as <code>versicolor</code>. Confusion matrix gives us information about the confusion of the classes by the model.</p>
</div>
<div id="naive-bayes-classification" class="section level2">
<h2>Naive Bayes Classification</h2>
<pre class="r"><code>require(e1071)
naiveModel &lt;- naiveBayes(Species~., data = trainIris) 
predClass &lt;- predict(naiveModel, testIris) 
confusionMatrix(testClass, predClass)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##             Reference
## Prediction   setosa versicolor virginica
##   setosa         17          0         0
##   versicolor      0         16         3
##   virginica       0          1        13
## 
## Overall Statistics
##                                           
##                Accuracy : 0.92            
##                  95% CI : (0.8077, 0.9778)
##     No Information Rate : 0.34            
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.8798          
##                                           
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: setosa Class: versicolor Class: virginica
## Sensitivity                   1.00            0.9412           0.8125
## Specificity                   1.00            0.9091           0.9706
## Pos Pred Value                1.00            0.8421           0.9286
## Neg Pred Value                1.00            0.9677           0.9167
## Prevalence                    0.34            0.3400           0.3200
## Detection Rate                0.34            0.3200           0.2600
## Detection Prevalence          0.34            0.3800           0.2800
## Balanced Accuracy             1.00            0.9251           0.8915</code></pre>
<p>This also has a pretty high accuracy.</p>
</div>
<div id="decision-trees" class="section level2">
<h2>Decision Trees</h2>
<pre class="r"><code>require(rpart)
decisionModel &lt;- rpart(Species~., data = trainIris) 
predClass &lt;- predict(decisionModel, testIris, type = &quot;class&quot;) 
confusionMatrix(testClass, predClass)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##             Reference
## Prediction   setosa versicolor virginica
##   setosa         17          0         0
##   versicolor      0         18         1
##   virginica       0          2        12
## 
## Overall Statistics
##                                           
##                Accuracy : 0.94            
##                  95% CI : (0.8345, 0.9875)
##     No Information Rate : 0.4             
##     P-Value [Acc &gt; NIR] : 8.745e-16       
##                                           
##                   Kappa : 0.909           
##                                           
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: setosa Class: versicolor Class: virginica
## Sensitivity                   1.00            0.9000           0.9231
## Specificity                   1.00            0.9667           0.9459
## Pos Pred Value                1.00            0.9474           0.8571
## Neg Pred Value                1.00            0.9355           0.9722
## Prevalence                    0.34            0.4000           0.2600
## Detection Rate                0.34            0.3600           0.2400
## Detection Prevalence          0.34            0.3800           0.2800
## Balanced Accuracy             1.00            0.9333           0.9345</code></pre>
<pre class="r"><code>require(rpart.plot)
prp(decisionModel) </code></pre>
<p><img src="classification_files/figure-html/unnamed-chunk-6-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Keep in mind that due to sampling your decision tree can look different than this.</p>
<p>This also has a very high accuracy; however, accuracy in itself is not sufficient in evaluating models. We should also consider the specificity and sensitivity values. Because, if there is an imbalance in the class, say you have 1000 <code>class A</code> in your test data and 10 <code>class B</code>. Then, if you label all the test data as <code>class A</code>, you will have high accuracy (1000/1010), however you weren’t able to detect any instances with <code>class B</code>, so your model is not very good. Sensitivity and Specificity both needs to be high for your model to be good.</p>
</div>
<div id="logistic-regression" class="section level2">
<h2>Logistic Regression</h2>
<p>Logistic regression is the type of regression where you fit a binary classification model. A binary classification model is the type of model where your output variable has 2 classes. Now this doesn?t mean that if you have a data that has more than one class (such as the iris data) cannot be modeled using a logistic regression. The idea is that you model each class vs. all others individually.</p>
<p>Let’s load the data:</p>
<pre class="r"><code>data &lt;- read.csv(url(&quot;http://archive.ics.uci.edu/ml/machine-learning-databases/cmc/cmc.data&quot;), sep=&quot;,&quot;, header = F) 
names(data) &lt;- c(&quot;w.age&quot;, &quot;w.ed&quot;, &quot;h.ed&quot;, &quot;child&quot;, &quot;rel&quot;,&quot;w.occ&quot;, &quot;h.occ&quot;, &quot;ind&quot;, &quot;med&quot;, &quot;outcome&quot;) 
data$w.ed &lt;- as.factor(data$w.ed) 
data$h.ed &lt;- as.factor(data$h.ed) 
data$rel&lt;-as.factor(data$rel) 
data$w.occ &lt;- as.factor(data$w.occ) 
data$h.occ &lt;- as.factor(data$h.occ) 
data$ind &lt;- as.factor(data$ind) 
data$med &lt;- as.factor(data$med) 
data$outcome &lt;- as.factor(data$outcome) 
summary(data)</code></pre>
<pre><code>##      w.age       w.ed    h.ed        child        rel      w.occ   
##  Min.   :16.00   1:152   1: 44   Min.   : 0.000   0: 220   0: 369  
##  1st Qu.:26.00   2:334   2:178   1st Qu.: 1.000   1:1253   1:1104  
##  Median :32.00   3:410   3:352   Median : 3.000                    
##  Mean   :32.54   4:577   4:899   Mean   : 3.261                    
##  3rd Qu.:39.00                   3rd Qu.: 4.000                    
##  Max.   :49.00                   Max.   :16.000                    
##  h.occ   ind     med      outcome
##  1:436   1:129   0:1364   1:629  
##  2:425   2:229   1: 109   2:333  
##  3:585   3:431            3:511  
##  4: 27   4:684                   
##                                  
## </code></pre>
<p>As you can see, we have 3 classes in <code>outcome</code> variable, which means that we have to generate 3 different models to perform logistic regression on this data for each class.</p>
<p>Let’s subset the data into training and testing sets:</p>
<pre class="r"><code>data &lt;- data[sample(nrow(data)),] #Shuffles the data by sampling nrow(data) observations from the data without replacement 
trainInd &lt;- round(nrow(data)*0.7) #Take 70% of data as training 
train &lt;- data[1:trainInd,] #Subset training data 
test.outcome &lt;- data[-(1:trainInd),10] #Separate the outcome values of test 
test &lt;- data[-(1:trainInd),-10] #Subset test data and remove outcome variable</code></pre>
<p>If you like, you can separate the training test further into training and validation tests to see if your model is working properly.</p>
<p>In R, we can train logistic regression with a single line of code. <code>glm</code> function computes logistic regression using <code>family = binomial(&quot;logit&quot;)</code> parameter. This means that our output variable has a binomial distribution of <code>1s</code> and <code>0s</code>. If you want to classify more that two outcomes, you will need to use two combinatorials of those outcomes (one vs. all). This is what we will try to do.</p>
<pre class="r"><code>iris2 &lt;-iris 
iris2$Species&lt;-as.numeric(iris2$Species)
#Create dataset for setosa 
iris2.setosa &lt;-iris2 
iris2.setosa$Species &lt;- as.factor(iris2.setosa$Species==1)
#Create dataset for versicolor 
iris2.versicolor &lt;-iris2 
iris2.versicolor$Species &lt;- as.factor(iris2.versicolor$Species==2) 
#Create dataset for virginica 
iris2.virginica &lt;-iris2 
iris2.virginica$Species &lt;- as.factor(iris2.virginica$Species==3)
logit.setosa &lt;- glm(Species~., data = iris2.setosa, family = binomial)
summary(logit.setosa)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Species ~ ., family = binomial, data = iris2.setosa)
## 
## Deviance Residuals: 
##        Min          1Q      Median          3Q         Max  
## -3.185e-05  -2.100e-08  -2.100e-08   2.100e-08   3.173e-05  
## 
## Coefficients:
##                Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept)     -16.946 457457.097       0        1
## Sepal.Length     11.759 130504.042       0        1
## Sepal.Width       7.842  59415.385       0        1
## Petal.Length    -20.088 107724.594       0        1
## Petal.Width     -21.608 154350.616       0        1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1.9095e+02  on 149  degrees of freedom
## Residual deviance: 3.2940e-09  on 145  degrees of freedom
## AIC: 10
## 
## Number of Fisher Scoring iterations: 25</code></pre>
<pre class="r"><code>class1.train &lt;- train 
class1.train$outcome &lt;- class1.train$outcome==1 #Get true for class = 1, false for otherwise 
class1.model &lt;- glm(outcome~., data = class1.train, family = binomial(&quot;logit&quot;)) 
summary(class1.model)</code></pre>
<pre><code>## 
## Call:
## glm(formula = outcome ~ ., family = binomial(&quot;logit&quot;), data = class1.train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.1713  -0.9243  -0.6321   1.0211   2.2409  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -0.27176    0.75711  -0.359 0.719638    
## w.age        0.09585    0.01207   7.939 2.04e-15 ***
## w.ed2       -0.18062    0.29641  -0.609 0.542283    
## w.ed3       -0.81008    0.30969  -2.616 0.008902 ** 
## w.ed4       -1.65666    0.33708  -4.915 8.89e-07 ***
## h.ed2       -1.08264    0.51410  -2.106 0.035214 *  
## h.ed3       -0.82134    0.50842  -1.615 0.106207    
## h.ed4       -0.60997    0.51601  -1.182 0.237167    
## child       -0.37069    0.04232  -8.759  &lt; 2e-16 ***
## rel1         0.44012    0.21508   2.046 0.040725 *  
## w.occ1      -0.10143    0.16422  -0.618 0.536818    
## h.occ2       0.07075    0.20256   0.349 0.726871    
## h.occ3       0.01953    0.19815   0.099 0.921491    
## h.occ4      -0.33759    0.54027  -0.625 0.532070    
## ind2        -0.42632    0.29255  -1.457 0.145049    
## ind3        -0.67529    0.27814  -2.428 0.015187 *  
## ind4        -0.97711    0.28143  -3.472 0.000517 ***
## med1         0.52648    0.29758   1.769 0.076860 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1404.7  on 1030  degrees of freedom
## Residual deviance: 1203.7  on 1013  degrees of freedom
## AIC: 1239.7
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>There are some irrelevant features in this model, so we can use stepwise removal to retain only relevant ones. There are other methods for variable selection which we will not cover in this tutorial.</p>
<pre class="r"><code>class1.model2 &lt;- step(class1.model, direction=&quot;backward&quot;, trace=0) 
summary(class1.model2)</code></pre>
<pre><code>## 
## Call:
## glm(formula = outcome ~ w.age + w.ed + h.ed + child + rel + ind + 
##     med, family = binomial(&quot;logit&quot;), data = class1.train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.1873  -0.9274  -0.6324   1.0338   2.2559  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -0.35145    0.68041  -0.517 0.605491    
## w.age        0.09632    0.01189   8.101 5.44e-16 ***
## w.ed2       -0.18290    0.29599  -0.618 0.536620    
## w.ed3       -0.79883    0.30856  -2.589 0.009628 ** 
## w.ed4       -1.64687    0.33130  -4.971 6.67e-07 ***
## h.ed2       -1.06912    0.51343  -2.082 0.037315 *  
## h.ed3       -0.79294    0.50638  -1.566 0.117369    
## h.ed4       -0.60218    0.51192  -1.176 0.239470    
## child       -0.37143    0.04204  -8.836  &lt; 2e-16 ***
## rel1         0.42518    0.21202   2.005 0.044925 *  
## ind2        -0.41257    0.29205  -1.413 0.157752    
## ind3        -0.67037    0.27780  -2.413 0.015818 *  
## ind4        -0.97163    0.27929  -3.479 0.000503 ***
## med1         0.52022    0.29639   1.755 0.079228 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1404.7  on 1030  degrees of freedom
## Residual deviance: 1204.7  on 1017  degrees of freedom
## AIC: 1232.7
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>After generating the model for outcome = 1, not we have to generate models for other outcome values.</p>
<p>For outcome = 2:</p>
<pre class="r"><code>class2.train &lt;- train 
class2.train$outcome &lt;- class2.train$outcome==2 #Get true for class = 1, false for otherwise 
class2.model &lt;- glm(outcome~., data = class2.train, family = binomial(&quot;logit&quot;)) 
summary(class2.model)</code></pre>
<pre><code>## 
## Call:
## glm(formula = outcome ~ ., family = binomial(&quot;logit&quot;), data = class2.train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.1965  -0.7641  -0.4818  -0.2246   2.6123  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -2.47659    0.91250  -2.714  0.00665 ** 
## w.age       -0.01031    0.01346  -0.766  0.44350    
## w.ed2        0.58038    0.50758   1.143  0.25286    
## w.ed3        1.44788    0.50513   2.866  0.00415 ** 
## w.ed4        2.07899    0.52458   3.963 7.40e-05 ***
## h.ed2       -0.77411    0.63479  -1.219  0.22267    
## h.ed3       -0.70800    0.59979  -1.180  0.23783    
## h.ed4       -0.58054    0.60021  -0.967  0.33343    
## child        0.21890    0.04495   4.870 1.12e-06 ***
## rel1        -0.37508    0.21545  -1.741  0.08170 .  
## w.occ1       0.06857    0.18945   0.362  0.71740    
## h.occ2      -0.38926    0.21290  -1.828  0.06749 .  
## h.occ3      -0.52045    0.21381  -2.434  0.01493 *  
## h.occ4       0.66120    0.60810   1.087  0.27689    
## ind2         0.11637    0.46505   0.250  0.80241    
## ind3         0.50810    0.42622   1.192  0.23322    
## ind4         0.59561    0.42714   1.394  0.16320    
## med1        -0.21911    0.44156  -0.496  0.61975    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1101.91  on 1030  degrees of freedom
## Residual deviance:  960.58  on 1013  degrees of freedom
## AIC: 996.58
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<pre class="r"><code>class2.model2 &lt;- step(class2.model, direction=&quot;backward&quot;, trace=0) 
summary(class2.model2)</code></pre>
<pre><code>## 
## Call:
## glm(formula = outcome ~ w.ed + child + rel + h.occ, family = binomial(&quot;logit&quot;), 
##     data = class2.train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.1424  -0.7440  -0.4936  -0.2493   2.6662  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -2.98779    0.52871  -5.651 1.59e-08 ***
## w.ed2        0.67053    0.46398   1.445 0.148409    
## w.ed3        1.57108    0.43795   3.587 0.000334 ***
## w.ed4        2.31572    0.44195   5.240 1.61e-07 ***
## child        0.20274    0.03569   5.680 1.35e-08 ***
## rel1        -0.38304    0.20672  -1.853 0.063893 .  
## h.occ2      -0.42307    0.20644  -2.049 0.040430 *  
## h.occ3      -0.55983    0.20420  -2.742 0.006115 ** 
## h.occ4       0.70071    0.59871   1.170 0.241852    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1101.91  on 1030  degrees of freedom
## Residual deviance:  967.44  on 1022  degrees of freedom
## AIC: 985.44
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>For outcome = 3:</p>
<pre class="r"><code>class3.train &lt;- train 
class3.train$outcome &lt;- class3.train$outcome==3 #Get true for class = 1, false for otherwise 
class3.model &lt;- glm(outcome~., data = class3.train, family = binomial(&quot;logit&quot;)) 
summary(class3.model)</code></pre>
<pre><code>## 
## Call:
## glm(formula = outcome ~ ., family = binomial(&quot;logit&quot;), data = class3.train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.6430  -0.9526  -0.6531   1.1890   2.5766  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -0.75860    0.95062  -0.798  0.42486    
## w.age       -0.09666    0.01270  -7.609 2.76e-14 ***
## w.ed2       -0.02523    0.30425  -0.083  0.93391    
## w.ed3        0.14206    0.31650   0.449  0.65353    
## w.ed4        0.44218    0.34043   1.299  0.19398    
## h.ed2        2.06639    0.77505   2.666  0.00767 ** 
## h.ed3        1.77870    0.77003   2.310  0.02089 *  
## h.ed4        1.54329    0.77502   1.991  0.04645 *  
## child        0.22245    0.03987   5.579 2.42e-08 ***
## rel1        -0.16090    0.21073  -0.764  0.44515    
## w.occ1       0.02203    0.16489   0.134  0.89374    
## h.occ2       0.31641    0.19823   1.596  0.11045    
## h.occ3       0.41813    0.19413   2.154  0.03126 *  
## h.occ4      -0.06386    0.57379  -0.111  0.91138    
## ind2         0.45603    0.29986   1.521  0.12831    
## ind3         0.49671    0.28398   1.749  0.08027 .  
## ind4         0.69378    0.28596   2.426  0.01526 *  
## med1        -0.44812    0.31236  -1.435  0.15140    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1336.5  on 1030  degrees of freedom
## Residual deviance: 1226.2  on 1013  degrees of freedom
## AIC: 1262.2
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<pre class="r"><code>class3.model2 &lt;- step(class3.model, direction=&quot;backward&quot;, trace=0) 
summary(class3.model2)</code></pre>
<pre><code>## 
## Call:
## glm(formula = outcome ~ w.age + h.ed + child + ind + med, family = binomial(&quot;logit&quot;), 
##     data = class3.train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.5831  -0.9630  -0.6793   1.1961   2.4086  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -0.46974    0.82868  -0.567  0.57081    
## w.age       -0.09521    0.01192  -7.991 1.34e-15 ***
## h.ed2        2.01441    0.76786   2.623  0.00871 ** 
## h.ed3        1.80020    0.75684   2.379  0.01738 *  
## h.ed4        1.65748    0.75293   2.201  0.02771 *  
## child        0.20733    0.03801   5.455 4.89e-08 ***
## ind2         0.46214    0.29726   1.555  0.12002    
## ind3         0.50193    0.28163   1.782  0.07471 .  
## ind4         0.69404    0.28000   2.479  0.01319 *  
## med1        -0.50715    0.29932  -1.694  0.09020 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1336.5  on 1030  degrees of freedom
## Residual deviance: 1236.1  on 1021  degrees of freedom
## AIC: 1256.1
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>In these models, p-values show the significance level of the variables. The residual deviance and null deviance show the variability of the residuals and the model predictions respectively. We want them to be as small as possible. Coefficients explain the effect of that variable on the outcome.</p>
<p>Now that we have generated our models, we can perform classification with the test set we have set aside:</p>
<pre class="r"><code>class1.test &lt;- predict(class1.model2, test, type = &quot;response&quot;) #Predicts probability of belonging to that class 
class2.test &lt;- predict(class2.model2, test, type = &quot;response&quot;) 
class3.test &lt;- predict(class3.model2, test, type = &quot;response&quot;) 
classProbs &lt;- cbind(class1.test, class2.test, class3.test) 
classProbs &lt;- classProbs/rowSums(classProbs) 
tclassProbs &lt;- data.frame(t(classProbs)) 
classes &lt;- as.factor(sapply(tclassProbs, which.max)) 
confusionMatrix(classes, test.outcome)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   1   2   3
##          1 120  33  62
##          2  11  35  18
##          3  62  32  69
## 
## Overall Statistics
##                                           
##                Accuracy : 0.5068          
##                  95% CI : (0.4591, 0.5543)
##     No Information Rate : 0.4367          
##     P-Value [Acc &gt; NIR] : 0.001784        
##                                           
##                   Kappa : 0.2178          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.001886        
## 
## Statistics by Class:
## 
##                      Class: 1 Class: 2 Class: 3
## Sensitivity            0.6218  0.35000   0.4631
## Specificity            0.6185  0.91520   0.6792
## Pos Pred Value         0.5581  0.54688   0.4233
## Neg Pred Value         0.6784  0.82804   0.7133
## Prevalence             0.4367  0.22624   0.3371
## Detection Rate         0.2715  0.07919   0.1561
## Detection Prevalence   0.4864  0.14480   0.3688
## Balanced Accuracy      0.6201  0.63260   0.5711</code></pre>
<p>This model obviously does not perform well. It can only predict the true class 50% of the time, which is better than chance level in this case because prediction true class out of 3 possible values has a chance value of 33% but still, 50% is not good. This process is also very difficult to do when working with more than 3 classes. With 4 classes, you need to generate 6 models. With 5 classes, you need to generate 10 models and with 10 classes you need to generate 45 models for classification. There are packages that do this for you but they are won’t be covered in this tutorial.</p>
<p>To evaluate a single logistic regression model, we can use the following code to get the p-value associated with it. Assume we want to test if class3.model2 is a viable model:</p>
<pre class="r"><code>with(class3.model2, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE))</code></pre>
<pre><code>## [1] 1.313437e-17</code></pre>
<p>The p-value is &lt;&lt;0.05, so the model is appropriate for use.</p>
<p>Suppose we want to see the probability and the confidence interval of beloging to that class by a random variable in the dataset for <code>class3.model2</code>, first we need to get the probabilities of that class along with the standard error of the prediction, then we plot it with the desired variable:</p>
<pre class="r"><code>newdata &lt;- cbind(test, predict(class3.model2, newdata = test, type = &quot;link&quot;,se = TRUE)) 
newdata &lt;- within(newdata, { 
  PredictedProb &lt;- plogis(fit) 
  LL &lt;- plogis(fit - (1.96 * se.fit)) 
  UL &lt;- plogis(fit + (1.96 * se.fit)) 
})</code></pre>
<p>Suppose we want to see how the probabilities change by <code>w.age</code>, the following code visualizes that relationship:</p>
<pre class="r"><code>ggplot(newdata, aes(x = w.age, y = PredictedProb)) + geom_ribbon(aes(ymin = LL,
                                                                     ymax = UL), alpha = 0.2) + geom_line(size = 1)</code></pre>
<p><img src="classification_files/figure-html/unnamed-chunk-19-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="support-vector-machines-svms" class="section level2">
<h2>Support Vector Machines (SVMs)</h2>
<p>SVMs can be used both for classification and regression. Luckily enough, you don’t have to explicitly perform one vs. all classification with SVMs. Even though SVMs perform one vs. all classification, they do this internally and we can simply use the <code>svm</code> function in <code>e1071</code> package. Let’s use the <code>iris</code> data set:</p>
<pre class="r"><code>svm.model &lt;- svm(Species~., data = trainIris) 
summary(svm.model)</code></pre>
<pre><code>## 
## Call:
## svm(formula = Species ~ ., data = trainIris)
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  radial 
##        cost:  1 
## 
## Number of Support Vectors:  43
## 
##  ( 18 18 7 )
## 
## 
## Number of Classes:  3 
## 
## Levels: 
##  setosa versicolor virginica</code></pre>
<pre class="r"><code>svm.preds &lt;- predict(svm.model, testIris) 
confusionMatrix(svm.preds, testClass)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##             Reference
## Prediction   setosa versicolor virginica
##   setosa         17          0         0
##   versicolor      0         18         1
##   virginica       0          1        13
## 
## Overall Statistics
##                                           
##                Accuracy : 0.96            
##                  95% CI : (0.8629, 0.9951)
##     No Information Rate : 0.38            
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.9395          
##                                           
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: setosa Class: versicolor Class: virginica
## Sensitivity                   1.00            0.9474           0.9286
## Specificity                   1.00            0.9677           0.9722
## Pos Pred Value                1.00            0.9474           0.9286
## Neg Pred Value                1.00            0.9677           0.9722
## Prevalence                    0.34            0.3800           0.2800
## Detection Rate                0.34            0.3600           0.2600
## Detection Prevalence          0.34            0.3800           0.2800
## Balanced Accuracy             1.00            0.9576           0.9504</code></pre>
<p>SVM performed well on <code>iris</code> dataset with the accuracy of 94%.</p>
</div>
<div id="artificial-neural-networks-anns" class="section level2">
<h2>Artificial Neural Networks (ANNs)</h2>
<p>ANNs can also be used for both regression and classification.</p>
<pre class="r"><code>require(nnet)
nnet.model &lt;- nnet(outcome~., data = train, size = 25) #size defines hidden layer size</code></pre>
<pre><code>## # weights:  528
## initial  value 1714.675831 
## iter  10 value 1101.888635
## iter  20 value 1088.043054
## iter  30 value 1044.792440
## iter  40 value 996.215018
## iter  50 value 933.153663
## iter  60 value 913.540607
## iter  70 value 900.309304
## iter  80 value 870.511859
## iter  90 value 850.791735
## iter 100 value 827.778323
## final  value 827.778323 
## stopped after 100 iterations</code></pre>
<pre class="r"><code>print(nnet.model)</code></pre>
<pre><code>## a 17-25-3 network with 528 weights
## inputs: w.age w.ed2 w.ed3 w.ed4 h.ed2 h.ed3 h.ed4 child rel1 w.occ1 h.occ2 h.occ3 h.occ4 ind2 ind3 ind4 med1 
## output(s): outcome 
## options were - softmax modelling</code></pre>
<pre class="r"><code>nnet.preds &lt;- predict(nnet.model, test) #Returns class probabilities, so we should perform some data cleaning as we did in the logistic regression
nnet.preds &lt;- nnet.preds/rowSums(nnet.preds) 
tnnet.preds &lt;- data.frame(t(nnet.preds)) 
classes &lt;- as.factor(sapply(tnnet.preds, which.max)) 
levels(classes) &lt;- c(&quot;1&quot;,&quot;2&quot;,&quot;3&quot;) 
confusionMatrix(classes, test.outcome)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   1   2   3
##          1 114  20  32
##          2  19  44  32
##          3  60  36  85
## 
## Overall Statistics
##                                           
##                Accuracy : 0.5498          
##                  95% CI : (0.5021, 0.5968)
##     No Information Rate : 0.4367          
##     P-Value [Acc &gt; NIR] : 1.161e-06       
##                                           
##                   Kappa : 0.3066          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.03232         
## 
## Statistics by Class:
## 
##                      Class: 1 Class: 2 Class: 3
## Sensitivity            0.5907  0.44000   0.5705
## Specificity            0.7912  0.85088   0.6724
## Pos Pred Value         0.6867  0.46316   0.4696
## Neg Pred Value         0.7138  0.83862   0.7548
## Prevalence             0.4367  0.22624   0.3371
## Detection Rate         0.2579  0.09955   0.1923
## Detection Prevalence   0.3756  0.21493   0.4095
## Balanced Accuracy      0.6909  0.64544   0.6214</code></pre>
<p>Neural Network performed on par with the logistic regression. However, neural network is very sensitive to hidden layer size. It is advised to use different sizes of hidden layers when generating neural network models.</p>
</div>
<div id="evaluation-measures" class="section level2">
<h2>Evaluation Measures</h2>
<p>There are several evaluation measures reported in the outputs of the models generated above. Three most important values are ?<em>Sensitivity</em>?, ?<em>Specificity</em>? and ?<em>Accuracy</em>?. Accuracy gives you which percent of the data you correctly classified. However this is not a good measure if there is a class unbalance. For instance, let?s say that you have 100 data points of which 95 are <code>class a</code> and 5 are <code>class b</code>. You can classify all 100 points as <code>class a</code> and you will still have 95% accuracy, even though you failed to find any data that belongs to <code>class b</code>. This is why we need sensitivity and specificity. Sensitivity tells us how many of the data that are of <code>class a</code>, we were able to classify as <code>class a</code>. In this example, this would be 1. Because every data that belonged to <code>class a</code> was classified as <code>class a</code>. Specificity tells us how many data points that were <code>class b</code> were classified as <code>class b</code>. In this example, specificity will be zero because none of the data that were <code>class b</code> was classified as such. Ideally, we want both of these values to be close to one. If all your values are close to one, then you have a good model.</p>
<div id="receiver-operating-characteristic-roc-curve-and-area-under-curve-auc" class="section level3">
<h3>Receiver Operating Characteristic (ROC) Curve and Area Under Curve (AUC)</h3>
<p>In a Receiver Operating Characteristic (ROC) curve the true positive rate (Sensitivity) is plotted in function of the false positive rate (100-Specificity) for different cut-off points. Each point on the ROC curve represents a sensitivity/specificity pair corresponding to a particular decision threshold. A test with perfect discrimination (no overlap in the two distributions) has a ROC curve that passes through the upper left corner (100% sensitivity, 100% specificity). Therefore the closer the ROC curve is to the upper left corner, the higher the overall accuracy of the test.</p>
<p>Let’s plot ROC curve of the <code>class3.model2</code> that we fit with logistic regression:</p>
<pre class="r"><code>#install.packages(pROC)
class3probs&lt;-predict(class3.model2,type=&quot;response&quot;)
require(pROC)
roccurve &lt;- roc(class3.model2$y, class3probs)
plot(roccurve)</code></pre>
<p><img src="classification_files/figure-html/unnamed-chunk-25-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>auc(roccurve)</code></pre>
<pre><code>## Area under the curve: 0.6789</code></pre>
<p>We want that curve to be far away from straight line. Ideally we want the area under the curve as high as possible. We want to make almost 0% mistakes while identifying all the positives, which means we want to see AUC value near to 1.</p>
<p>As can be seen, the AUC for logistic regression model of class 3 is 0.66. It is a fairly good model but it can be enhanced.</p>
</div>
</div>
<div id="useful-links" class="section level2">
<h2>Useful Links</h2>
<ul>
<li>Caret package documentation: <a href="http://www.jstatsoft.org/v28/i05/paper" class="uri">http://www.jstatsoft.org/v28/i05/paper</a>
<ul>
<li>This webpage holds examples and advanced methods for generating both classification and regression models using <code>caret</code> package.</li>
</ul></li>
<li>Accurately determining prediction error: <a href="http://scott.fortmann-roe.com/docs/MeasuringError.html" class="uri">http://scott.fortmann-roe.com/docs/MeasuringError.html</a>
<ul>
<li>This document explains the details of error measurements</li>
</ul></li>
<li>Multinomial logit model website: <a href="http://cran.r-project.org/web/packages/mlogit/vignettes/mlogit.pdf" class="uri">http://cran.r-project.org/web/packages/mlogit/vignettes/mlogit.pdf</a>
<ul>
<li>This website contains examples and usage details of the <code>mlogit</code> package</li>
</ul></li>
</ul>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
