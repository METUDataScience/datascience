<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Regression Analysis</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Introduction to Data Science</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Tutorials
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="installation.html">R/RStudio Installation</a>
    </li>
    <li>
      <a href="azure_notebooks.html">Azure Notebooks</a>
    </li>
    <li>
      <a href="introduction_to_r.html">Introduction to R</a>
    </li>
    <li>
      <a href="amelia.html">Missing Data Imputation</a>
    </li>
    <li>
      <a href="associationMining.html">Association Mining</a>
    </li>
    <li>
      <a href="classification.html">Classification</a>
    </li>
    <li>
      <a href="regression.html">Regression Analysis</a>
    </li>
    <li>
      <a href="timeSeries.html">Time Series Analysis</a>
    </li>
    <li>
      <a href="visualization.html">Visualization</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://mehmetaliakyol.com">
    <span class="fa fa-question fa-lg"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Regression Analysis</h1>

</div>


<p><strong>Objectives</strong>:</p>
<p>The objective of this document is to give a brief introduction to linear regression. This document assumes the users have no prior knowledge of R. After completing this tutorial, you will be able to:</p>
<ul>
<li>Model data using simple linear regression</li>
<li>Model data using multiple linear regression</li>
<li>Diagnose and evaluate regression models</li>
<li>Make predictions based on selected model</li>
</ul>
<p>First, we will cover simple linear regression and then we will proceed to performing multivariate linear regression.</p>
<p>First, we need to set the data into appropriate format:</p>
<pre class="r"><code>data &lt;- read.table(&quot;http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data&quot;, 
                   header = F) 
names(data) &lt;- c(&quot;crim&quot;, &quot;zn&quot;, &quot;indus&quot;, &quot;chas&quot;, &quot;nox&quot;, &quot;rm&quot;, &quot;age&quot;,&quot;dis&quot;,&quot;rad&quot;,&quot;tax&quot;,&quot;ptratio&quot;,&quot;b&quot;,&quot;lstat&quot;,&quot;medv&quot;)</code></pre>
<p>In this dataset, <code>medv</code> is the variable we want to use as the outcome variable and the rest are possible predictors.</p>
<p>To perform accurate linear regression analysis, first you must perform exploratory data analysis. This is the case for every type of statistical analysis, but in this case, variable relations are important for the initial formulation of model.</p>
<p>First we need to take a look at the correlations. Correlations can only be computed using numeric variables, so we change our factor variables back to numeric form.</p>
<pre class="r"><code>cor.values &lt;- cor(data) 
View(cor.values)</code></pre>
<p>Based on the correlation values, we can see that <code>medv</code> has a strong positive correlation with <code>rm</code> variable and a strong negative correlation with <code>lstat</code>.</p>
<p>We might also want to visualize the relationships between variables. To do that, we use scatterplot matrices. Let’s generate a scatterplot matrix using <code>GGally</code> package’s <code>ggpairs</code> function. Of course, we have a lot of variables so visualizing all of them may not be a viable option. So let’s only visualize <code>medv</code>, <code>rm</code> and <code>lstat</code> relationships.</p>
<pre class="r"><code>#install.packages(&quot;GGally&quot;) 
require(GGally)
ggpairs(data, columns = c(6, 13, 14))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-3-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The plots in the diagonal are the density plots of the variables while other plots are the scatterplots of those variables. Recall that scatterplots were used to visualize the relationship between variables. The upper part of the diagonal shows the correlation between variables.</p>
<p>As you can see, relationship between <code>rm</code> and <code>medv</code> is sufficiently linear looking, while the relationship between <code>lstat</code> and <code>medv</code> is slightly non-linear. This might be due to the effect of other variables or it can simply be the nature of their relationship. To determine which is true, you need to perform further exploratory data analysis which is not in the scope of this tutorial.</p>
<p>Let’s assume we want to model the relationship between <code>medv</code> and <code>rm</code> initially.</p>
<div id="simple-linear-regression" class="section level3">
<h3>Simple Linear Regression</h3>
<p>Simple linear regression is the regression in which there is only one predictor variable and one outcome variable. Intercept is not considered a variable since it is a constant.</p>
<pre class="r"><code>slr &lt;- lm(medv~rm, data = data) 
summary(slr)</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ rm, data = data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -23.346  -2.547   0.090   2.986  39.433 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -34.671      2.650  -13.08   &lt;2e-16 ***
## rm             9.102      0.419   21.72   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 6.616 on 504 degrees of freedom
## Multiple R-squared:  0.4835, Adjusted R-squared:  0.4825 
## F-statistic: 471.8 on 1 and 504 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>require(ggplot2)
qplot(x = rm, y= medv, data = data) + geom_abline(intercept = coef(slr)[1], 
                                                  slope = coef(slr)[2], 
                                                  col = &quot;red&quot;, size = 1.5)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-5-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>As you can see from the summary of our model, both the intercept and the coefficient of predictor variable are statistically significant. However, while evaluating the performance of our model, we should also consider the adjusted R^2 to see how much of the variance in the outcome value is explained by our model. It turns out that our model can only explain 48.25% of the variability in the outcome so it is not sufficient.</p>
<p>Also, to evaluate the <em>correctness</em> of our model, we should take a look at the diagnostic plots. Diagnostic plots give us an information about the behavior of residuals. Ideally, after contructing a model, we want our residuals to be randomly and normally distributed with <code>mu$ = 0</code>. In other words, we want our residuals to be independent and normal. We also want our residuals to have constant variance.</p>
<pre class="r"><code>par(mfrow = c(2,2)) 
plot(slr)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-6-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The plot on the top left (Residuals vs. Fitted) gives us information about the relationship between the fitted values and the residuals. We want this to be randomly distributed, however it is not. When we move from left to right on x-axis, we can see that the variance of our residuals get lower. This means there is a relationship between the fitted values and the residuals. This plot also shows the possible outliers in our data that might be affecting our model. The numbers given on the points in the plot are the index numbers of values that possibly change our model significantly.</p>
<p>The plot on the top right (Q-Q plot) tests if our residuals are randomly distributed. Turns out, some parts of our residuals are randomly distributed but not all of them. Again, the numbers indicate possibly problematic observations.</p>
<p>The plot on the lower left (Scale-Location Plot) shows if our residuals have constant variance. The red line shows the approximated behavior of our residuals and we want this line to me mostly flat.</p>
<p>Our final plot (Residuals vs. Leverage) explicitly explores which observations have the most effect on our model. The observations that have high Cook’s distance might be affecting our model badly.</p>
<p>So let’s re-fit the model after removing those problematic points.</p>
<pre class="r"><code>data3 &lt;- data[-c(365,366,369,373),] 
slr2 &lt;- lm(medv~rm, data = data3) 
summary(slr2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ rm, data = data3)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -19.8376  -2.2858   0.3701   3.0871  28.3087 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -40.3182     2.4763  -16.28   &lt;2e-16 ***
## rm            9.9758     0.3915   25.48   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.985 on 500 degrees of freedom
## Multiple R-squared:  0.565,  Adjusted R-squared:  0.5641 
## F-statistic: 649.3 on 1 and 500 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>qplot(x = rm, y= medv, data = data3) + geom_abline(intercept = coef(slr2)[1], 
                                                   slope = coef(slr2)[2], 
                                                   col = &quot;red&quot;, size = 1.5)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-8-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Our adjusted R^2 increased. Obviously those points were affecting the model.</p>
<pre class="r"><code>par(mfrow = c(2,2)) 
plot(slr2)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-9-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>However we still have some issues. Let’s perform one more iteration.</p>
<pre class="r"><code>data4 &lt;- data3[-c(368,370,372,407),] 
slr3 &lt;- lm(medv~rm, data = data4) 
summary(slr3)</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ rm, data = data4)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -19.8460  -2.2810   0.3656   3.0623  28.3233 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -40.5845     2.4532  -16.54   &lt;2e-16 ***
## rm           10.0163     0.3878   25.83   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.881 on 496 degrees of freedom
## Multiple R-squared:  0.5735, Adjusted R-squared:  0.5727 
## F-statistic:   667 on 1 and 496 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>qplot(x = rm, y= medv, data = data4) + geom_abline(intercept = coef(slr3)[1], 
                                                   slope = coef(slr3)[2], 
                                                   col = &quot;red&quot;, size = 1.5)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-11-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Our adjusted R^2 increased. Obviously those points were affecting the model.</p>
<pre class="r"><code>par(mfrow = c(2,2)) 
plot(slr3)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-12-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>It is getting better by iteration, but we might also consider building our model with a different predictor. But for now, this tutorial will advance to explain how multivariate linear regression works. We will explain model selection at the end of this tutorial.</p>
<p>###Multivariate Linear Regression</p>
<p>If we know which variables we want to add to the model, we can use the <code>update</code> function. Note that we want our model to have linearly independent predictors in it because linearly dependent predictors cause non-constant variance in the model, also known as heteroscedasticity. To determine which variables we might add, we can look at our correlations again and choose variables that have low correlation with <code>rm</code> and relatively high correlation with <code>medv</code>. Let’s use <code>tax</code> variable.</p>
<pre class="r"><code>mlr &lt;- update(slr3, medv~rm+tax) 
summary(mlr)</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ rm + tax, data = data4)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -14.770  -2.884  -0.402   2.290  32.691 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -25.831425   2.526591  -10.22   &lt;2e-16 ***
## rm            8.764234   0.361243   24.26   &lt;2e-16 ***
## tax          -0.017023   0.001474  -11.54   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.226 on 495 degrees of freedom
## Multiple R-squared:  0.664,  Adjusted R-squared:  0.6626 
## F-statistic: 489.1 on 2 and 495 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>As you can see, our adjusted R^2 increased again. Let’s check our model diagnostics:</p>
<pre class="r"><code>par(mfrow = c(2,2)) 
plot(mlr)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-14-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>We still have some problems remaining; let’s remove those points once more.</p>
<pre class="r"><code>data5 &lt;- data4[-c(368,370,372),] 
mlr2 &lt;- lm(medv~rm+tax, data = data5) 
summary(mlr2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ rm + tax, data = data5)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -14.6244  -2.7950  -0.3736   2.3454  28.7427 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -25.682057   2.430549  -10.57   &lt;2e-16 ***
## rm            8.759124   0.347101   25.23   &lt;2e-16 ***
## tax          -0.017414   0.001427  -12.20   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.013 on 492 degrees of freedom
## Multiple R-squared:  0.6855, Adjusted R-squared:  0.6842 
## F-statistic: 536.3 on 2 and 492 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>par(mfrow = c(2,2)) 
plot(mlr2)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-16-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Turns out, we have some problems in our data. It could be due to the data collection method or the variables simply might not have a linear relationship.</p>
<p>Let’s generate another model with three predictors, and this time let’s use a factor predictor. When using factors, one thing you need to keep in mind that, <code>lm</code> treats each factor level as a seperate dummy variable and includes <code>(n-1)</code> variables into the model. Because including <code>(n-1)</code> variables is enough to represent the factor relationship of <code>n</code> levels. The left out factor level is simply any data that does not belong to <code>(n-1)</code> factor levels. If you want <code>lm</code> to include all factor variables, then you should remove the intercept by adding a <code>-1</code> term to the formula.</p>
<pre class="r"><code>## (n-1) levels 
mlr3 &lt;- update(mlr2, medv~rm+tax+as.factor(rad)) 
summary(mlr3)</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ rm + tax + as.factor(rad), data = data5)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -14.4329  -2.8303  -0.3718   2.6993  28.9269 
## 
## Coefficients:
##                    Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)      -28.373637   2.942927  -9.641  &lt; 2e-16 ***
## rm                 8.685298   0.358727  24.211  &lt; 2e-16 ***
## tax               -0.014414   0.003524  -4.090 5.06e-05 ***
## as.factor(rad)2    1.207273   1.505716   0.802  0.42307    
## as.factor(rad)3    3.629331   1.380558   2.629  0.00884 ** 
## as.factor(rad)4    1.335693   1.222005   1.093  0.27492    
## as.factor(rad)5    3.337217   1.210385   2.757  0.00605 ** 
## as.factor(rad)6    1.703802   1.506320   1.131  0.25857    
## as.factor(rad)7    2.129403   1.637502   1.300  0.19408    
## as.factor(rad)8    2.704402   1.509479   1.792  0.07382 .  
## as.factor(rad)24   1.002395   1.767517   0.567  0.57090    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.96 on 484 degrees of freedom
## Multiple R-squared:  0.697,  Adjusted R-squared:  0.6908 
## F-statistic: 111.4 on 10 and 484 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>## n levels 
mlr4 &lt;- update(mlr2, medv~rm+tax+as.factor(rad)-1) 
summary(mlr4)</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ rm + tax + as.factor(rad) - 1, data = data5)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -14.4329  -2.8303  -0.3718   2.6993  28.9269 
## 
## Coefficients:
##                    Estimate Std. Error t value Pr(&gt;|t|)    
## rm                 8.685298   0.358727  24.211  &lt; 2e-16 ***
## tax               -0.014414   0.003524  -4.090 5.06e-05 ***
## as.factor(rad)1  -28.373637   2.942927  -9.641  &lt; 2e-16 ***
## as.factor(rad)2  -27.166364   2.885519  -9.415  &lt; 2e-16 ***
## as.factor(rad)3  -24.744306   2.737838  -9.038  &lt; 2e-16 ***
## as.factor(rad)4  -27.037944   2.716110  -9.955  &lt; 2e-16 ***
## as.factor(rad)5  -25.036419   2.789139  -8.976  &lt; 2e-16 ***
## as.factor(rad)6  -26.669835   2.911277  -9.161  &lt; 2e-16 ***
## as.factor(rad)7  -26.244234   3.029846  -8.662  &lt; 2e-16 ***
## as.factor(rad)8  -25.669235   3.048830  -8.419 4.32e-16 ***
## as.factor(rad)24 -27.371242   3.483481  -7.857 2.56e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.96 on 484 degrees of freedom
## Multiple R-squared:  0.9586, Adjusted R-squared:  0.9576 
## F-statistic:  1018 on 11 and 484 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Diagnostic plot of (n-1) factor levels:</p>
<pre class="r"><code>par(mfrow = c(2,2)) 
plot(mlr3)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-19-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Diagnostic plot of (n) factor levels:</p>
<pre class="r"><code>par(mfrow = c(2,2)) 
plot(mlr4)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-20-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>As you can see, diagnostic plots are the same for both models, because they are essentially the same model. However the adjusted R^2 values are different. This suggests that the intercept has a huge effect on the model accuracy. You could examine the effect of intercept by removing it from previous models.</p>
</div>
<div id="model-selection" class="section level3">
<h3>Model Selection</h3>
<div id="anova" class="section level4">
<h4>ANOVA</h4>
<p>After generating our models, we need to determine which one we want to use for prediction. To do that, we evaluate their difference using the <code>anova</code> function. <code>anova</code> function requires the observation numbers to be the same between models so let’s refit all the previous models using the first 300 observations. I choose to omit last 206 observations because data seemed problematic in that area. This number of observations is arbitrarily selected at the moment, when performing your analyses you should be careful regarding which observations to omit.</p>
<pre class="r"><code>data.final = data[1:300,] 
slr.final &lt;- lm(medv~rm, data = data.final) 
mlr.final &lt;- lm(medv~rm+tax, data = data.final) 
mlr2.final &lt;- lm(medv~rm+tax+as.factor(rad), data = data.final) 
mlr3.final &lt;- lm(medv~rm+tax+as.factor(rad)-1, data = data.final)#No intercept 

#Perform anova 
anova(slr.final, mlr.final, mlr2.final, mlr3.final)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Model 1: medv ~ rm
## Model 2: medv ~ rm + tax
## Model 3: medv ~ rm + tax + as.factor(rad)
## Model 4: medv ~ rm + tax + as.factor(rad) - 1
##   Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    
## 1    298 4570.5                                  
## 2    297 4305.6  1    264.84 19.875 1.183e-05 ***
## 3    290 3864.2  7    441.38  4.732 4.751e-05 ***
## 4    290 3864.2  0      0.00                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Anova tests if the newly added variables are statistically significant. Since there is no change in variables between <code>mlr2.final</code> and <code>mlr3.final</code>, no value is reported for that test. Remember, only the representation changes between those two models.</p>
<p>Anova takes the smallest model as the base line and tests for significance of new variables. So the first entry of the anova table is <code>slr.final</code> model and since it is the baseline, no significance value is reported. Second model is <code>mlr.final</code> and only one more variable is included after the baseline so the degrees of freedom (Df) is stated as 1. The added variable seems statistically significant. For the third model, the newly added 8 factor levels are treated as dummy variables by the <code>lm</code> function so the degress of freedom is 8. It is also significant.</p>
<p>You can also use <code>anova</code> with completely different models (i.e. without using update function)</p>
<p>However, this is a very cumbersome process and there are other measures to use in model selection, such as AIC or BIC. Luckily, there are automated methods we can use instead of using anova. However, keep in mind that anova is a more thorough way of model selection while stepwise selection is the fast way to do model selection.</p>
</div>
<div id="stepwise-selection" class="section level4">
<h4>Stepwise Selection</h4>
<p>Stepwise selection is performed by adding/removing variables one step at a time and measuring their performance. <code>MASS</code> package has functions that can do automatic model selection.</p>
<pre class="r"><code>#install.packages(&quot;MASS&quot;) 
require(MASS)
fit &lt;- lm(medv~., data = data.final) 
step &lt;- stepAIC(fit, direction=&quot;both&quot;, trace = 0) 
summary(step)</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ crim + nox + rm + age + dis + tax + ptratio + 
##     b + lstat, data = data.final)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.9564 -2.0033 -0.2851  1.7666 11.6146 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -12.629276   5.143782  -2.455 0.014666 *  
## crim          1.213392   0.486932   2.492 0.013264 *  
## nox          -8.283596   3.986613  -2.078 0.038602 *  
## rm            9.272261   0.399553  23.207  &lt; 2e-16 ***
## age          -0.048528   0.010490  -4.626 5.62e-06 ***
## dis          -0.924692   0.155674  -5.940 8.15e-09 ***
## tax          -0.012429   0.003069  -4.050 6.57e-05 ***
## ptratio      -0.686352   0.097584  -7.033 1.45e-11 ***
## b             0.016876   0.005010   3.368 0.000858 ***
## lstat        -0.104799   0.052077  -2.012 0.045103 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.177 on 290 degrees of freedom
## Multiple R-squared:  0.876,  Adjusted R-squared:  0.8722 
## F-statistic: 227.7 on 9 and 290 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Diagnostic plot:</p>
<pre class="r"><code>par(mfrow = c(2,2)) 
plot(step)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-23-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>By looking at the diagnostics plot, we can see that most of the residuals are randomly and normally distributed and there is no heteroscedasticity. We should also compare this with other possible models based on BIC and adjusted R^2. We could use <code>leaps</code> package for that.</p>
<pre class="r"><code>#install.packages(&quot;leaps&quot;) 
library(leaps)
leaps &lt;- regsubsets(medv~., data = data.final, nbest = 1, nvmax = 10) #Model can have maximum 10 variables and retain only the best model of each subset 
layout(matrix(1:2, ncol = 2)) 
plot(leaps,scale=&quot;bic&quot;) 
plot(leaps,scale=&quot;adjr2&quot;)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-24-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>These plots show the included variables in the model and that model’s BIC and adjusted R^2 values. As you can see the selected model has average BIC over all possible models but it has the second best adjusted R^2. The automatic selection optimizes the IC and adjusted R^2 values.</p>
<p>We have our final model in <code>step</code> variable. We need to interpret the coefficients.</p>
</div>
</div>
<div id="model-interpretation" class="section level3">
<h3>Model Interpretation</h3>
<pre class="r"><code>summary(step)</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ crim + nox + rm + age + dis + tax + ptratio + 
##     b + lstat, data = data.final)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.9564 -2.0033 -0.2851  1.7666 11.6146 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -12.629276   5.143782  -2.455 0.014666 *  
## crim          1.213392   0.486932   2.492 0.013264 *  
## nox          -8.283596   3.986613  -2.078 0.038602 *  
## rm            9.272261   0.399553  23.207  &lt; 2e-16 ***
## age          -0.048528   0.010490  -4.626 5.62e-06 ***
## dis          -0.924692   0.155674  -5.940 8.15e-09 ***
## tax          -0.012429   0.003069  -4.050 6.57e-05 ***
## ptratio      -0.686352   0.097584  -7.033 1.45e-11 ***
## b             0.016876   0.005010   3.368 0.000858 ***
## lstat        -0.104799   0.052077  -2.012 0.045103 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.177 on 290 degrees of freedom
## Multiple R-squared:  0.876,  Adjusted R-squared:  0.8722 
## F-statistic: 227.7 on 9 and 290 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The intercept is the value <code>medv</code> takes when all other predictors are zero. In this case, this values is -12.63, and it doesn’t make much sense because median value of the house cannot be negative. Also, this value has a very high standard error (5.14) and this means that out estimate may not be very accurate. Those reasons might be why models with intercept performed poorly. The second coefficient tells us that there is 1.21k dollars increase in median value of the house with each point increase in crime rate per capita when all other values are constant, with a standard error of 0.49. Again, this doesn’t sound very intuitive but consider the opposite: crime rate is higher in neighborhoods where there are expensive houses. So the correlation might indicate that relationship, however this needs to be investigated. The median value of the house drops by 8.28k dollars with each point increase in nitric oxide concentration per ten million with high standard error of 3.99, which means that the prices vary a lot with respect to this variable. This interpretation makes sense. Again, the median value of the house increases 9.27k dollars with each additional room, with very little standard error. This is also a sensible deduction. It tells us that number of rooms is a big factor in the median value of house and this does not vary a lot. To fully understand the effects on outcome, you might also want to fit another model without the intercept to see if there is a change.</p>
<pre class="r"><code>fit.noint &lt;- lm(medv~.-1, data = data.final) 
step.noint &lt;- stepAIC(fit.noint, direction=&quot;both&quot;, trace = 0) 
summary(step.noint)</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ crim + nox + rm + age + dis + tax + ptratio + 
##     b + lstat - 1, data = data.final)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.0477 -2.0858 -0.3989  1.9065 11.5257 
## 
## Coefficients:
##           Estimate Std. Error t value Pr(&gt;|t|)    
## crim      1.321940   0.489092   2.703  0.00728 ** 
## nox     -13.957958   3.276304  -4.260 2.76e-05 ***
## rm        8.608280   0.296653  29.018  &lt; 2e-16 ***
## age      -0.044473   0.010448  -4.257 2.80e-05 ***
## dis      -1.107012   0.137999  -8.022 2.57e-14 ***
## tax      -0.013401   0.003069  -4.366 1.76e-05 ***
## ptratio  -0.822766   0.080914 -10.168  &lt; 2e-16 ***
## b         0.011954   0.004631   2.581  0.01033 *  
## lstat    -0.140942   0.050383  -2.797  0.00549 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.205 on 291 degrees of freedom
## Multiple R-squared:  0.9863, Adjusted R-squared:  0.9859 
## F-statistic:  2331 on 9 and 291 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>There is not much difference between models in terms of interpretation. Except there is no intercept which didn’t make much sense. The rest of the variables are interpreted in a similar way with the previous model.</p>
<p>There is a huge difference regarding adjusted R^2 and using the second model might be better for prediction.</p>
</div>
<div id="prediction" class="section level3">
<h3>Prediction</h3>
<p>R has a function <code>predict</code> for predicting the outcome for new observation.</p>
<pre class="r"><code>new.obs &lt;- data.frame(cbind(0.03, 0.42, 8, 56, 5, 302, 17, 350, 6.72)) #Random values for prediction 
names(new.obs) &lt;- names(step.noint$coefficients) #Unless you give the names, predict cannot recognize values 
outcome.pred &lt;- predict(step.noint, new.obs, interval = &quot;prediction&quot;) #Gives prediction intervals
outcome.conf &lt;- predict(step.noint, new.obs, interval = &quot;confidence&quot;) #Gives confidence intervals of the prediction 
outcome.pred #fit gives you the fitted value, lwr and upr give you the lower and upper bounds</code></pre>
<pre><code>##        fit      lwr      upr
## 1 40.22078 33.80574 46.63582</code></pre>
<pre class="r"><code>outcome.conf</code></pre>
<pre><code>##        fit      lwr     upr
## 1 40.22078 39.05106 41.3905</code></pre>
</div>
<div id="useful-links" class="section level2">
<h2>Useful Links</h2>
<ul>
<li>Swirl Student Page: <a href="http://swirlstats.com/students.html" class="uri">http://swirlstats.com/students.html</a>
<ul>
<li>This web site includes details of the R package <code>swirl</code>. <code>Swirl</code> is a package that allows you to learn R in an interactive way.</li>
</ul></li>
</ul>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
